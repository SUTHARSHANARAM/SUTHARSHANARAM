{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObzSP2PuBCySF/nCZPeMQu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SUTHARSHANARAM/SUTHARSHANARAM/blob/main/Spark%20program%20to%20perform%20classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqNR2shxBwD4",
        "outputId": "ec60e37b-6499-4546-ee1a-d71f0bdb4018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------+-----+\n",
            "|feature1|feature2|label|\n",
            "+--------+--------+-----+\n",
            "|     1.0|     2.0|    0|\n",
            "|     2.0|     1.5|    0|\n",
            "|     3.0|     3.5|    1|\n",
            "|     4.0|     5.0|    1|\n",
            "|     5.0|     3.0|    1|\n",
            "|     6.0|     6.0|    1|\n",
            "|     7.0|     8.0|    1|\n",
            "|     0.5|     1.0|    0|\n",
            "|     1.5|     2.5|    0|\n",
            "+--------+--------+-----+\n",
            "\n",
            "+---------+-----+----------+--------------------+\n",
            "| features|label|prediction|         probability|\n",
            "+---------+-----+----------+--------------------+\n",
            "|[3.0,3.5]|    1|       0.0|[0.54281284112144...|\n",
            "|[5.0,3.0]|    1|       1.0|[7.52656390841942...|\n",
            "+---------+-----+----------+--------------------+\n",
            "\n",
            "Coefficients: [6.585983453790336,7.011767177400992]\n",
            "Intercept: -44.470807222059115\n",
            "Accuracy: 0.5\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"ClassificationExample\").getOrCreate()\n",
        "\n",
        "# Sample data (replace or expand as needed)\n",
        "data = [\n",
        "    Row(feature1=1.0, feature2=2.0, label=0),\n",
        "    Row(feature1=2.0, feature2=1.5, label=0),\n",
        "    Row(feature1=3.0, feature2=3.5, label=1),\n",
        "    Row(feature1=4.0, feature2=5.0, label=1),\n",
        "    Row(feature1=5.0, feature2=3.0, label=1),\n",
        "    Row(feature1=6.0, feature2=6.0, label=1),\n",
        "    Row(feature1=7.0, feature2=8.0, label=1),\n",
        "    Row(feature1=0.5, feature2=1.0, label=0),\n",
        "    Row(feature1=1.5, feature2=2.5, label=0)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()\n",
        "\n",
        "# Step 1: Assemble features into a single vector column\n",
        "assembler = VectorAssembler(inputCols=[\"feature1\", \"feature2\"], outputCol=\"features\")\n",
        "df = assembler.transform(df)\n",
        "\n",
        "# Step 2: Prepare the data for training\n",
        "# Select only the 'features' and 'label' columns\n",
        "df = df.select(\"features\", \"label\")\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=123)\n",
        "\n",
        "# Check if test data is empty\n",
        "if test_data.count() == 0:\n",
        "    print(\"Test data is empty. Adjusting split...\")\n",
        "    train_data, test_data = df.randomSplit([0.9, 0.1], seed=123)\n",
        "\n",
        "# Step 4: Define and fit the logistic regression model\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "lr_model = lr.fit(train_data)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "test_predictions = lr_model.transform(test_data)\n",
        "test_predictions.select(\"features\", \"label\", \"prediction\", \"probability\").show()\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "# Print the coefficients and intercept of the model\n",
        "print(\"Coefficients:\", lr_model.coefficients)\n",
        "print(\"Intercept:\", lr_model.intercept)\n",
        "\n",
        "# Step 7: Model evaluation using MulticlassClassificationEvaluator\n",
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", labelCol=\"label\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(test_predictions)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Stop Spark session\n",
        "spark.stop()\n",
        "\n"
      ]
    }
  ]
}